# -*- coding: utf-8 -*-
"""Thanos_LSTM_test.ipynb

Automatically generated by Colaboratory.
Last part of code was excecuted in google Colaboratory due to perfomance issues
#Libraries and mounting
"""

import numpy as np
import tensorflow as tf
import random as rn
import os
#os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"] = ""
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(42)
rn.seed(12345)
tf.random.set_seed(1234)
session_conf = tf.compat.v1.ConfigProto(
    intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from tensorflow import keras
from tensorflow.keras import backend as K
sess = tf.compat.v1.Session(
    graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import nltk
import string
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, BatchNormalization, SpatialDropout1D, GRU, GlobalAveragePooling1D, Dense, Dropout, concatenate
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.initializers import Constant
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
import tensorflow_hub as hub
import logging
from IPython.display import display, HTML
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.utils import class_weight
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from gensim.models import Word2Vec, FastText
logging.basicConfig(level=logging.INFO)
from sklearn.preprocessing import label_binarize
from itertools import cycle
from sklearn.metrics import roc_curve, auc
from scipy import interp

import random 
import math
import seaborn as sns

import io
from google.colab import drive 
 
#Import Training data  
drive.mount("/content/drive") 

train = pd.read_csv('/content/drive/MyDrive/got_balanced.csv')
#test = pd.read_csv('/content/drive/My Drive/thesis_sotiria/got_classes_no_list.csv')
train = train.sample(frac =1)



#TRAIN DATA
X = train.loc[:, 'tweets_2'].values.astype(str)
y = train.loc[:, 'cred_score_classes'].values

#TEST DATA

#X_test = test.loc[:, 'tweets_2'].values
#y_test = test.loc[:, 'final_cred_score'].values

train.head()

train["cred_score_classes"].value_counts()

train.head()





"""#Simple_LSTM"""

X,y

#1
#Simple LSTM

#Train-test data split
#X_train_val, X_test, y_train, y_test = train_test_split(
#    X, y, test_size=0.2, random_state=101) 

#Train-validation data split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=101)

depth= 5
y_one= tf.one_hot(y, depth)
y_train= tf.one_hot(y_train, depth)
y_val= tf.one_hot(y_val, depth)


tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
#pad_sequences
##columns = [train['tweets_2'], test['tweets_2']]

#text = pd.concat(columns)
max_length = max([len(s.split()) for s in train['tweets_2'].astype(str)])
print(max_length)
#pad_sequences
#columns = [train['Preprocessed_sentences'], test['Preprocessed_sentences']]

#text = pd.concat(columns)
#max_length = max([len(s.split()) for s in text])

#define vocabulary size
vocab_size = len(tokenizer.word_index) + 1


X_train_tokens = tokenizer.texts_to_sequences(X_train)

X_train_pad = pad_sequences(X_train_tokens, 
                maxlen= max_length, padding= 'post')

X_val_tokens = tokenizer.texts_to_sequences(X_val)

X_val_pad = pad_sequences(X_val_tokens, 
                maxlen= max_length, padding= 'post')

#X_test_tokens = tokenizer.texts_to_sequences(X_test)

#X_test_pad = pad_sequences(X_test_tokens, 
#                maxlen= max_length, padding= 'post')

#Build Model

embedding_dim = 200

print('Building model...')

K.clear_session()

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, 
                    input_length = max_length))
model.add(SpatialDropout1D(0.25))
model.add(LSTM(180, dropout=0.5, return_sequences=True))
model.add(BatchNormalization())
model.add(LSTM(90, dropout=0.5, return_sequences=True))
model.add(BatchNormalization())
model.add(LSTM(40, dropout=0.5))
model.add(BatchNormalization())
model.add(Dense(20, activation='relu'))

model.add(Dense(5, activation='softmax'))
opt = keras.optimizers.Adam(learning_rate=0.005)
model.compile(loss='categorical_crossentropy',optimizer=opt, 
                           metrics=['accuracy'])
print(model.summary())

# plot graph
display(plot_model(model, show_shapes=True))

#Training Model

print('Train...')

y_train_labels = np.argmax(y_one, axis =1)
class_weights = class_weight.compute_class_weight(
    'balanced', np.unique(y_train_labels), y_train_labels)
class_weight_dict = dict(enumerate(class_weights))
#early_stop = EarlyStopping(monitor='val_loss', mode= 'min', verbose= 1,  patience = 5)
      
model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), 
          batch_size= 64, epochs= 50,
          class_weight=class_weight_dict, shuffle=False)

#Printing losses
model_loss = pd.DataFrame(model.history.history)
model_loss[['val_loss',"loss"]].plot()
[['val_loss',"loss"]]

model.save('/content/drive/My Drive/thesis_sotiria/simple_lstm_test')


X_test_tokens = tokenizer.texts_to_sequences(X_test)

X_test_pad = pad_sequences(X_test_tokens, 
                maxlen= max_length, padding= 'post')




"""#Text and Numerical Data combined"""

train.head()

train.columns



#Text and Numerical Data combined

text = train.loc[:, 'tweets_2'].values
X_mixed_train = train.loc[:, {'tweets_2','user_favorites', 'listed_count',
                  'statuses_count','verified',
                  'ADJ', 'ADP', 'ADV', 'CCONJ', 'DET',
       'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB',
       'X', 'SPACE'}]

y = train.loc[:, 'cred_score_classes'].values


#Train-test data split
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101) 

#print(X_train.shape, X_test.shape, y_train.shape)

#Divide train dataset between text and numerical data
X1_train = X_mixed_train.loc[:, 'tweets_2'].values    #text data - train
X2_train = X_mixed_train.loc[:, ['user_favorites', 'listed_count',
                  'statuses_count','verified',
                   'ADJ', 'ADP', 'ADV', 'CCONJ', 'DET',
       'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB',
       'X', 'SPACE']].values    #numerical data - train



X1_train = X1_train.astype(str)

#y_train = y_train.values
#y_test = y_test.values

depth= 5
y_train_ohe= tf.one_hot(y, depth)
print(y_train_ohe)

#Scale numerical data between [0, 1]
scaler_object = MinMaxScaler()
scaler_object.fit(X2_train)
scaled_X2_train = scaler_object.transform(X2_train)  #scaled numerical train data


#Tokenizer
tokenizer = Tokenizer(oov_token=True)
tokenizer.fit_on_texts(X1_train)

#pad_sequences
max_length = max([len(s.split()) for s in train['tweets_2'].astype(str)])

#define vocabulary size
vocab_size = len(tokenizer.word_index) + 1

X1_train_tokens = tokenizer.texts_to_sequences(X1_train)

X1_train_pad = pad_sequences(X1_train_tokens, 
                maxlen= max_length, padding= 'post')



#define word index
word_index = tokenizer.word_index

numerical_columns = len(list(X_mixed_train))-1


# Build the model 

embedding_dim = 300

#text data input
text_input = Input(shape=(max_length), name= 'Text_data') 
#emb = embedding_layer (text_input)
emb = Embedding(vocab_size, embedding_dim, input_length = max_length) (text_input)
spat_drop = SpatialDropout1D(0.25)(emb) 
#lstm1 = Bidirectional(LSTM(120, dropout=0.5))(spat_drop) 
lstm1 = LSTM(240, dropout=0.4, return_sequences= True)(spat_drop)
drop1 = Dropout(0.2)(lstm1)

lstm2 = LSTM(240, dropout=0.4, return_sequences= True)(drop1) 
drop2 = Dropout(0.2)(lstm2)

lstm3 = LSTM(120, dropout=0.4, return_sequences= True)(drop2) 
drop3 = Dropout(0.2)(lstm3)

lstm4 = LSTM(120, dropout=0.4, return_sequences= True)(drop3) 
drop4 = Dropout(0.2)(lstm4)

lstm5 = LSTM(60, dropout=0.4, return_sequences= True)(drop4) 
drop5 = Dropout(0.2)(lstm5)

lstm6 = LSTM(60, dropout=0.4, return_sequences= True)(drop5) 
drop6 = Dropout(0.2)(lstm6)

lstm7 = LSTM(30, dropout=0.4, return_sequences= True)(drop6) 
drop7 = Dropout(0.2)(lstm7)

lstm8 = LSTM(30, dropout=0.4, return_sequences= True)(drop7) 
drop8 = Dropout(0.2)(lstm8)

lstm9 = LSTM(15, dropout=0.4, return_sequences= True)(drop8) 
drop9 = Dropout(0.2)(lstm9)

lstm10 = LSTM(15, dropout=0.4)(drop9) 
drop10 = Dropout(0.2)(lstm10)






#out1 = Dense(20, activation='tanh')(lstm1)

#numerical data input
metadata = Input(shape=(numerical_columns), name= 'Numerical_data')
hidden = Dense(100, activation='relu')(metadata)
hidden2 = Dense(100, activation='relu')(hidden)
hidden3 = Dense(80, activation='relu')(hidden2)
hidden4 = Dense(80, activation='relu')(hidden3)
hidden5 = Dense(60, activation='relu')(hidden4)
hidden6 = Dense(50, activation='relu')(hidden5)
hidden7 = Dense(40, activation='relu')(hidden6)
hidden8 = Dense(30, activation='relu')(hidden7)
hidden9 = Dense(20, activation='relu')(hidden8)
hidden10 = Dense(15, activation='relu')(hidden9)


out2 = Dense(10, activation='relu')(hidden10)

#Concatenation of two outputs
concat = concatenate([drop10, out2])  # merge output of text and numerical data

output = Dense(5, activation='softmax')(concat) 
model = Model(inputs=[text_input, metadata], outputs=output)

# Compile the model 
model.compile(loss='categorical_crossentropy',optimizer='Adam', 
                           metrics=['accuracy'])

print(model.summary())

# plot graph
display(plot_model(model))

#Training Model

print('Train...')
y_train_labels = np.argmax(y_train_ohe, axis =1)
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'text_numerical.h5', monitor='val_accuracy', save_best_only=True, verbose=2)


model.fit(x= [X1_train_pad, scaled_X2_train], 
          y= y_train_ohe, validation_split=0.4, batch_size= 64,
          epochs= 30)

#Printing losses
model_loss = pd.DataFrame(model.history.history)
model_loss[['val_loss',"loss"]].plot()
[['val_loss',"loss"]]

model_loss[['val_accuracy',"accuracy"]].plot()